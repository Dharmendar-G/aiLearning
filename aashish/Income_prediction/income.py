# -*- coding: utf-8 -*-
"""income.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qFFUBxdvn1s5YV-SAUNdi6LGM4il1XSn

**Importing All Required Libraries**
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import re
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import f1_score, accuracy_score
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import confusion_matrix

"""**Importing the Dataset**"""

columns = ['Age', 'Work Class', 'Final Weight', 'Education', 'Education Number', 'Marital Status', 'Occupation',
          'Relationship', 'Race', 'Sex', 'Capital Gain', 'Capital Loss', 'Hours per Week', 'Country', 'Income']
df=pd.read_csv(r"/content/Income_data.csv",names=columns,skiprows=1)

"""**Exploring the Dataset**"""

df.head()

df.info()

"""Getting some basic mathematical info about data"""

df.describe()

"""Checking for null values"""

df.isnull().sum()

"""**Correlation matrix between attributes**"""

sns.heatmap(df.corr(),annot=True)

"""We can see that there are no null values in dataset. Now we will check the value counts."""

for i in df.columns:
  print("Value counts of "+i,end="\n\n")
  print(df[i].value_counts(),end="\n\n\n\n\n")

"""**Transformating Data**

**Age Column :** we can see that age has integer values  and between 17 and 90.
so nothing has to be changed
"""

sns.distplot(df.Age)

"""**Work class Column :** there are nan values so we need to replace it with mode. """

# for i,j in enumerate(df["Work Class"]):
#    if not re.match(r"[A-Za-z]*",j):
#       print(j)
#       df.loc[i]["Work Class"]=np.nan
df["Work Class"].hist()
plt.xticks(rotation=90)

most_frequent_category=df["Work Class"].mode()[0]
df["Work Class"].fillna(most_frequent_category,inplace=True)

"""**Final Weight Column :** the value count in final weight column is too high so we can exclude this column."""

df.drop(["Final Weight"],inplace=True,axis=1)

"""**Education Column :** From value counts we can see that Education and Education-Number column are both same but education is having categorical data of standard and Education-Number is having numerical data so we can drop the Education column which is categorical."""

df.drop(["Education"],inplace=True,axis=1)

"""**Occupation Column :** we have to fill values having Nan with mode"""

sns.countplot(df["Occupation"])
plt.xticks(rotation=90)
plt.show()
most_frequent_cat=df["Occupation"].mode()[0]
df["Occupation"].fillna(most_frequent_cat,inplace=True)

"""**Country column :** We can observe that united states is maximum occuring so we are replacing all other as Other_Country."""

df["Country"].hist()
plt.xticks(rotation=90)
plt.show()

most_frequent_cat=df["Country"].mode()[0]
df["Country"].fillna(most_frequent_cat,inplace=True)

arr=list(df["Country"].unique())
arr=arr[1:]
df["Country"].replace(arr,"Other_Countries",inplace=True)

"""After replacing with all other countries other than united_states"""

sns.countplot(df["Country"])
plt.show()

"""**Label Encoding the Income column**"""

sns.countplot(df["Income"])
plt.show()

labelEncoder = LabelEncoder()
df['Income'] = labelEncoder.fit_transform(df['Income'])

"""After label Encoding"""

df["Income"].hist()

"""**Exploratory Data Analysis**

**Visualising the  value counts of all the columns with count plots**
"""

for i in df.columns:
  print("Count plot for "+i)
  sns.countplot(x=df[i],data=df)
  plt.xticks(rotation=90)
  plt.show()
  print("\n\n\n")

"""**Pair Plot**"""

sns.pairplot(df)
plt.show()

"""**Encoding the categorical values**"""

target=df["Income"]
new_df=df.drop(["Income"],axis=True)
new_df=pd.get_dummies(new_df)

"""**Modelling**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(new_df, target, test_size = 0.30, random_state = 10)

classifiers = [GaussianNB(),  
               DecisionTreeClassifier(random_state = 0), 
               RandomForestClassifier(n_estimators = 100, random_state = 10), 
               GradientBoostingClassifier(random_state = 0)]
classifier_names = ["Gaussian Naive Bayes", 
                    "Decision Tree Classifier", 
                    "Random Forest Classifier", 
                    "Gradient Boosting Classifier"]
accuracies = []
confu=[]

for i in range(len(classifiers)):
    classifier = classifiers[i]
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)
    print("{}:".format(classifier_names[i]))
    print("F1 score: {:.2f}".format(f1_score(y_test, y_pred)))
    accuracy = accuracy_score(y_test, y_pred)*100
    accuracies.append(accuracy)
    cm = confusion_matrix(y_test, y_pred)
    confu.append(cm)

for v in range(len(classifier_names)):
  plt.clf()
  plt.imshow(confu[v], interpolation='nearest', cmap=plt.cm.Pastel1)
  classNames = ['Negative','Positive']
  plt.title(classifier_names[v])
  plt.ylabel('True label')
  plt.xlabel('Predicted label')
  tick_marks = np.arange(len(classNames))
  plt.xticks(tick_marks, classNames, rotation=45)
  plt.yticks(tick_marks, classNames)
  s = [['TN','FP'], ['FN', 'TP']]
  for i in range(2):
    for j in range(2):
        plt.text(j,i, str(s[i][j])+" = "+str((confu[v])[i][j]))
  plt.show()

accuracies

